The results of this Chapter highlight the importance of the file types chosen to compose the dataset. File types that hold compressed data or images are harder do classify. This suggests that their entropy is a source of classification errors. This also may explain why previous works in the field have achieved different results but the ``CLD'' model was able to almost match their results, as shown in Figure 4.3.

\subsection{Comparison with other works}

A comparison was made between a chosen model, ``CLD'', and recent works in the field. While ``CLD'' achieved slight lower accuracy values than the other studies, it can be noticed that the values are similar:
while the result values of these other works range from 61\% to 98\%, a difference of 37\%, the differences between the results from this study and theirs do not exceed 7\%. This is an indication that the high  variability that these studies show between each other is caused by the difference in the file types they choose. This supports the claim that advances in this area should come from error analysis.

The Govdocs1 dataset brought an important basis for comparing different carving solutions. But to achieve easily reproducible results, the models must also be publicly available, a condition not all revised studies fulfill. Being available as Jupyter notebooks at https://github.com/atilaromero/carving-experiments, the results described here should require little effort to be reproduced. Thus the source code can be used as a basis of comparison in future researches, generating models with the same architecture of those presented here.

\subsection{Accuracy of pairs of classes}

The accuracy of models trained with pairs of classes, shown in Figure \ref{fig:dual}, suggests a reverse correlation between entropy and accuracy. Generally, file types with higher entropy tend to have lower minima, with the GIF file type being a notable exception. Most of these files use some form of compression, as image files for example.

It was demonstrated that the accuracy of a new model may be manipulated by the selection of file types that will compose the dataset. The lines ``hard file types first'' and ``easy file types first'' of Figure \ref{fig:nclasses} were created using the order shown in Figure \ref{fig:dual}, resulting in lines that seem to be close to the minimum and maximum of the possible accuracy values. 

\subsection{Accuracy vs. number of classes}

In Figure \ref{fig:nclasses}, a decreasing trend was observed. An increase in the number of classes appears to be correlated with a decrease in accuracy. Another relevant aspect of the graph is that the range of the results seems to be smaller when more classes are used.  

This pattern is understandable: as the number of classes grows, the harder the classification problem is, leading to a decrease in accuracy. Meanwhile, the individual contributions of each class to the overall result diminish, leading to a decrease in variation between the different results for a given number of classes.

This behavior is an important aspect to consider during the evaluation of file fragments studies. This observation is in agreement with Beebe \textit{et al.} \cite{beebe_sceadan:_2013} observation, that the studies that select fewer classes tend to yield higher results. 

Still, with 42\% \footnote{In the extended training session described in Chapter 4, a higher accuracy was obtained and 38\% of samples were misclassified, instead of 42\%.} of samples being misclassified when the number of classes is 28, the question of what are the error sources and how they can be addressed requires attention.

The number of possible combinations of file types to compose the datasets depends on the number of classes being considered. For 28 classes, there is only one possible combination, while for two classes there are 378. For intermediary values, the numbers are much higher, which is the number of possible combinations disregarding the order of the elements: $ \frac{28!}{(28-n)!n!}$. For 14 file types, there are 40,116,600 combinations. For this reason, the significance of the 5 samples diminishes for intermediary values.

