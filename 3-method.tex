TODO: rewrite
this chapter describes several neural network training sessions using different sets of file types and the attempt to understand how the accuracy of the resulting models are affected by composition of file types in the set.

\subsection{Comparison with other works}
The model ``CLD'' was chosen to be compared with other works because it was among the three models with the highest accuracy, but achieved its results in less time and showed high resilience to changes in parameters during the tests, while the others did not.

In a second stage, a longer training session was performed, using a batch size of 300 samples instead of 100, and increasing the improvement threshold of the early stopping condition (maximum number of epochs to wait for improvements) from 10 to 20. These parameters were chosen by trial and error. For the initial 28 file types, the ``CLD'' model training time increased from eight minutes to three hours using these parameters.

Using longer training sessions, the accuracy for the 28 file types used in the first stage increased from 54\% to 63\%. Using the same file types of other works, the ``CLD'' model achieved an accuracy of 
67\% for the file types from Chen \textit{et al.} \cite{chen_file_2018},
91\% for Hiester \cite{hiester_file_2018}, 
59\% for Wang \textit{et al.} \cite{wang_sparse_2018},
65\% for Wang \textit{et al.} \cite{wang_file_2018},
and
61\% for VulinoviÄ‡ \textit{et al.} \cite{vulinovic_neural_2019}.

\subsection{Accuracy of pairs of classes}

In addition to this sampling of extensions using different quantities of classes, all 378 combinations of pairs of extensions were compared. Each pair was used to compose a dataset and to train a new model for each one, using the same process described in the previous section.

Given the number of different models trained (135 for the N classes phase and 378 for the pairs), it was important to use a model architecture that had a fast training time. Models trained with the chosen architecture took about 10 minutes to train. Without this short training period, this research part would not be feasible.



\subsection{Accuracy vs. number of classes}
Several independent models with the same general architecture, but different weights, were trained with 2 to 28 extensions from the Govdocs1 dataset. For each number in the range 2 to 28, a random subset of extensions was selected to compose the dataset. Each extension has 200 file samples, half placed in the training dataset and half in the validation dataset. Each dataset was used to train a new model, that should distinguish between the selected subsets of extensions in that filtered dataset. This process was repeated five times.

\subsection{Model}
TODO: CLD

The model architecture used in this research stage, ``CLD'', is described in the previous chapter and has three layers. The first is a convolutional layer with 256 output units, a window of size 16, and a stride of 16. The second is an LSTM layer of 128 units. The last is a fully-connected layer where the number of output units matches the number of classes of the input dataset.
The same dataset handling, sampling, output, and input encoding described in Chapter 4 were used here.

%optimization
All the training sessions used the Adam \cite{kingma_adam:_2014}
optimization algorithm to guide backpropagation, which was selected because it performed well in the preliminary tests without requiring fine-tuning of parameters.

% Constraints
TODO: different contraints for each experiment
The models were trained until no further improvement was observed in the last 10 epochs, using categorical cross-entropy loss.

\subsection{Hardware and software}
The main software and frameworks used to build the experiments were Python 3.6 
\cite{rossum_python_2019}, Jupyter notebook \cite{perez_jupyter_2019}, Tensorflow 1.14.0 \cite{google_brain_tensorflow_2019}, Keras 2.2.4-tf \cite{chollet_keras_2019}, and Fedora Linux 27.
% repository
The source code for the experiments is available at \sloppy\url{http://github.com/atilaromero/carving-experiments}.

% \levelB{Hardware}
The experiments did not take advantage of GPU acceleration and were  conducted on a single computer with 256GB of RAM and with 2 Intel\textregistered Xeon\textregistered E5-2630 v2 processors, with 6 cores each, with 2 hyper-threads per core, or 24 hyper-threads in total. 

